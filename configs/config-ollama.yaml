# OpenRAG Configuration for Ollama
# This config uses Ollama for local LLM inference

vector_store:
  type: qdrant
  qdrant:
    host: localhost
    port: 6333
    collection_name: openrag
    vector_size: 384
    distance_metric: cosine
    api_key: null

embedding:
  model_name: sentence-transformers/all-MiniLM-L6-v2
  batch_size: 32
  device: cpu
  normalize_embeddings: true

llm:
  provider: ollama
  ollama:
    host: http://localhost
    port: 11434
    model: llama3
    temperature: 0.7
    max_tokens: 1000
    timeout: 60
    system_prompt: null

chunking:
  strategy: fixed
  chunk_size: 500
  chunk_overlap: 50

retrieval:
  top_k: 5
  min_similarity: 0.0

api:
  host: 0.0.0.0
  port: 8000
  reload: false
  workers: 1
  cors_origins:
    - "*"

logging:
  level: INFO
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: null
